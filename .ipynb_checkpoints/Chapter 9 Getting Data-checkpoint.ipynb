{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# egrep.py\n",
    "\n",
    "import sys, re\n",
    "\n",
    "#sys.argv is the list of command-libe arguments \n",
    "#sys.argv[0] is the name of the program itself\n",
    "#sys.argv[1] will be the regex specified at the cmd line\n",
    "regex = sys.argv[1]\n",
    "\n",
    "#for every line passed into the script\n",
    "for line in sys.stdin:\n",
    "    #if it matches the regex, write it to stdout\n",
    "    if re.search(regex, line):\n",
    "        sys.stdout(line)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#line_count.py\n",
    "\n",
    "import sys\n",
    "\n",
    "count = 0 \n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "    \n",
    "# print goes to sys.stdout\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this one is mainly psuedocode, lots of undefined variables\n",
    "\n",
    "# \"r\" means read only\n",
    "file_for_reading = open('reading_file.txt', 'r')\n",
    "\n",
    "# 'w' means write only -- will destroy the file if it alraedy exists!\n",
    "file_for_writing = open(\"writing_file.txt\", 'w')\n",
    "\n",
    "# 'a' is append\n",
    "file_for_appending = open(\"appending_file.txt\",\"a\")\n",
    "\n",
    "#dont forget to close your files when you're done\n",
    "file_for_writing.close()\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = function_that_gets_data_from(f)\n",
    "#at this point f has been closed, so don't try to use it\n",
    "\n",
    "process(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "starts_with_hash = 0\n",
    "\n",
    "with open('input.txt',\"r\") as f:      \n",
    "    for line in f:                  \n",
    "        if re.match(\"^#\", line):\n",
    "            starts_with_hash += 1\n",
    "        \n",
    "print starts_with_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Barnett Sailcloth\\Documents\\Projects\\Python\\Data Science From Scratch\n",
      "['.ipynb_checkpoints', 'afreemansworship.txt', 'boxplot.html', 'categorical_scatter_jitter.html', 'Chapter 8 Gradient Descent.ipynb', 'Chapter 9 Getting Data.ipynb', 'Crash course in python.py', 'egrep.py', 'hello.py', 'input.txt.txt', 'Introduction.py', 'Jimmy Jams.ipynb', 'Key connectors p3.py', 'line_count.py', 'lorenz.html', 'most_common_words.py', 'some_text.txt', 'test.png', 'untitled']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print os.getcwd()\n",
    "print os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html = requests.get(\"http://thebrowser.com\").text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "# print soup\n",
    "\n",
    "first_paragraph = soup.find('p')\n",
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "# print first_paragraph_text\n",
    "\n",
    "first_paragraph_id = soup.p.get('id')\n",
    "\n",
    "all_spans = soup.find_all('span')\n",
    "paragraphs = [p for p in soup('p', \"content clearfix\")]\n",
    "# print paragraphs\n",
    "# print all_spans\n",
    "# print paragraphs_with_ids\n",
    "\n",
    "time_span = soup('span',{'class':'time-to-read'})\n",
    "# print time_span\n",
    "\n",
    "spans_inside_divs = [span for div in soup(\"div\") for span in div(\"span\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-2edaa4f4f602>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-2edaa4f4f602>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    addr =\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "addr = \n",
    "headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"}\n",
    "\n",
    "\n",
    "oreilly_html = requests.get(addr, headers=headers).text\n",
    "oreilly_soup = BeautifulSoup(oreilly_html, 'html5lib')\n",
    "\n",
    "print oreilly_soup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'publicationYear': u'2014', u'Author': u'Joel Grus', u'topics': [u'data', u'science', u'data science'], u'title': u'Data Science Book'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_ex = {\"title\" : \"Data Science Book\",\n",
    " \"Author\": \"Joel Grus\",\n",
    " \"publicationYear\" : \"2014\",\n",
    " \"topics\" : [\"data\", \"science\", \"data science\"]\n",
    "}\n",
    "\n",
    "serialised = \"\"\"{\"title\" : \"Data Science Book\",\n",
    " \"Author\": \"Joel Grus\",\n",
    " \"publicationYear\" : \"2014\",\n",
    " \"topics\" : [\"data\", \"science\", \"data science\"]}\"\"\"\n",
    "\n",
    "#parse the JSON to create a Python dict\n",
    "deserialised = json.loads(serialised)\n",
    "if \"data science\" in deserialised[\"topics\"]:\n",
    "    print deserialised\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Python', u'Python', u'Python', u'Python', u'Jupyter Notebook', u'JavaScript', u'HTML', u'Python', u'Jupyter Notebook', u'Python', u'Jupyter Notebook', u'JavaScript', u'HTML', u'PureScript', u'JavaScript', u'Elm', u'HTML', u'HTML', u'JavaScript', u'Python', u'F#', u'JavaScript', u'Clojure', u'F#', u'JavaScript', None, u'JavaScript', u'Python', u'Python', u'Ruby']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "from collections import Counter\n",
    "\n",
    "endpoint = \"https://api.github.com/users/joelgrus/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "last_5_repositories = sorted(repos, key=lambda r: r[\"created_at\"], reverse=True)\n",
    "last_5_languages = [repo[\"language\"] for repo in last_5_repositories]\n",
    "\n",
    "print last_5_languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named twython",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-5d48867b71ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtwython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTwython\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcons_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"KAe4iZiJ9KL41yruyqHxYdmsQ\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcons_secret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"BkvR3UT45MY120sBl2rMk7Z8jkW68cwa7DpGeWWp7RkISSQHbh\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named twython"
     ]
    }
   ],
   "source": [
    "from twython import Twython\n",
    "\n",
    "cons_key = \"KAe4iZiJ9KL41yruyqHxYdmsQ\"\n",
    "\n",
    "cons_secret = \"BkvR3UT45MY120sBl2rMk7Z8jkW68cwa7DpGeWWp7RkISSQHbh\"\n",
    "\n",
    "access_token =  \"946604745828872192-5Ec8uJUmSFn5PcKkPrghf66cYnHAlIR\"\n",
    "\n",
    "secret_token = \"QefYwFpT5H6YZLWkV38YyLUAFisldqCz79WjTmVbENmfC\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
