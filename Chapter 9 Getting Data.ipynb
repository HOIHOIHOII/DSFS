{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# egrep.py\n",
    "\n",
    "import sys, re\n",
    "\n",
    "#sys.argv is the list of command-libe arguments \n",
    "#sys.argv[0] is the name of the program itself\n",
    "#sys.argv[1] will be the regex specified at the cmd line\n",
    "regex = sys.argv[1]\n",
    "\n",
    "#for every line passed into the script\n",
    "for line in sys.stdin:\n",
    "    #if it matches the regex, write it to stdout\n",
    "    if re.search(regex, line):\n",
    "        sys.stdout(line)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#line_count.py\n",
    "\n",
    "import sys\n",
    "\n",
    "count = 0 \n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "    \n",
    "# print goes to sys.stdout\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this one is mainly psuedocode, lots of undefined variables\n",
    "\n",
    "# \"r\" means read only\n",
    "file_for_reading = open('reading_file.txt', 'r')\n",
    "\n",
    "# 'w' means write only -- will destroy the file if it alraedy exists!\n",
    "file_for_writing = open(\"writing_file.txt\", 'w')\n",
    "\n",
    "# 'a' is append\n",
    "file_for_appending = open(\"appending_file.txt\",\"a\")\n",
    "\n",
    "#dont forget to close your files when you're done\n",
    "file_for_writing.close()\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = function_that_gets_data_from(f)\n",
    "#at this point f has been closed, so don't try to use it\n",
    "\n",
    "process(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "starts_with_hash = 0\n",
    "\n",
    "with open('input.txt',\"r\") as f:      \n",
    "    for line in f:                  \n",
    "        if re.match(\"^#\", line):\n",
    "            starts_with_hash += 1\n",
    "        \n",
    "print starts_with_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Barnett Sailcloth\\Documents\\Projects\\Python\\Data Science From Scratch\n",
      "['.ipynb_checkpoints', 'afreemansworship.txt', 'boxplot.html', 'categorical_scatter_jitter.html', 'Chapter 8 Gradient Descent.ipynb', 'Chapter 9 Getting Data.ipynb', 'Crash course in python.py', 'egrep.py', 'hello.py', 'input.txt.txt', 'Introduction.py', 'Jimmy Jams.ipynb', 'Key connectors p3.py', 'line_count.py', 'lorenz.html', 'most_common_words.py', 'some_text.txt', 'test.png', 'untitled']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print os.getcwd()\n",
    "print os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html = requests.get(\"http://thebrowser.com\").text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "# print soup\n",
    "\n",
    "first_paragraph = soup.find('p')\n",
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "# print first_paragraph_text\n",
    "\n",
    "first_paragraph_id = soup.p.get('id')\n",
    "\n",
    "all_spans = soup.find_all('span')\n",
    "paragraphs = [p for p in soup('p', \"content clearfix\")]\n",
    "# print paragraphs\n",
    "# print all_spans\n",
    "# print paragraphs_with_ids\n",
    "\n",
    "time_span = soup('span',{'class':'time-to-read'})\n",
    "# print time_span\n",
    "\n",
    "spans_inside_divs = [span for div in soup(\"div\") for span in div(\"span\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-2edaa4f4f602>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-2edaa4f4f602>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    addr =\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "addr = \n",
    "headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"}\n",
    "\n",
    "\n",
    "oreilly_html = requests.get(addr, headers=headers).text\n",
    "oreilly_soup = BeautifulSoup(oreilly_html, 'html5lib')\n",
    "\n",
    "print oreilly_soup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'publicationYear': u'2014', u'Author': u'Joel Grus', u'topics': [u'data', u'science', u'data science'], u'title': u'Data Science Book'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_ex = {\"title\" : \"Data Science Book\",\n",
    " \"Author\": \"Joel Grus\",\n",
    " \"publicationYear\" : \"2014\",\n",
    " \"topics\" : [\"data\", \"science\", \"data science\"]\n",
    "}\n",
    "\n",
    "serialised = \"\"\"{\"title\" : \"Data Science Book\",\n",
    " \"Author\": \"Joel Grus\",\n",
    " \"publicationYear\" : \"2014\",\n",
    " \"topics\" : [\"data\", \"science\", \"data science\"]}\"\"\"\n",
    "\n",
    "#parse the JSON to create a Python dict\n",
    "deserialised = json.loads(serialised)\n",
    "if \"data science\" in deserialised[\"topics\"]:\n",
    "    print deserialised\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Python', u'Python', u'Python', u'Python', u'Jupyter Notebook', u'JavaScript', u'HTML', u'Python', u'Jupyter Notebook', u'Python', u'Jupyter Notebook', u'JavaScript', u'HTML', u'PureScript', u'JavaScript', u'Elm', u'HTML', u'HTML', u'JavaScript', u'Python', u'F#', u'JavaScript', u'Clojure', u'F#', u'JavaScript', None, u'JavaScript', u'Python', u'Python', u'Ruby']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "from collections import Counter\n",
    "\n",
    "endpoint = \"https://api.github.com/users/joelgrus/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "last_5_repositories = sorted(repos, key=lambda r: r[\"created_at\"], reverse=True)\n",
    "last_5_languages = [repo[\"language\"] for repo in last_5_repositories]\n",
    "\n",
    "print last_5_languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "himlohiya : RT @AnalyticsVidhya: Here're 28 cheat sheets for #MachineLearning, #Datascience, Probability, SQL &amp; Big Data - pin them and use them for yo‚Ä¶\n",
      "\n",
      "BioDataScience : Governance in Data Science https://t.co/kozjM915gk\n",
      "\n",
      "184plus1 : RT @DD_Jessica_: Data Science from Scratch: First Principles with Python https://t.co/XOQBljFxcm\n",
      "\n",
      "DD_Jessica_ : Data Science from Scratch: First Principles with Python https://t.co/XOQBljFxcm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from twython import Twython\n",
    "\n",
    "cons_key = \"KAe4iZiJ9KL41yruyqHxYdmsQ\"\n",
    "\n",
    "cons_secret = \"BkvR3UT45MY120sBl2rMk7Z8jkW68cwa7DpGeWWp7RkISSQHbh\"\n",
    "\n",
    "access_token =  \"946604745828872192-5Ec8uJUmSFn5PcKkPrghf66cYnHAlIR\"\n",
    "\n",
    "secret_token = \"QefYwFpT5H6YZLWkV38YyLUAFisldqCz79WjTmVbENmfC\"\n",
    "\n",
    "twitter = Twython(cons_key,cons_secret)\n",
    "\n",
    "# search for tweets containing the phrase \"data science\"\n",
    "for status in twitter.search(q='\"data science\"')[\"statuses\"]:\n",
    "    user = status[\"user\"][\"screen_name\"].encode('utf-8')\n",
    "    text = status[\"text\"].encode('utf-8')\n",
    "    print user, \":\", text\n",
    "    print\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recieved tweet # 1\n",
      "recieved tweet # 2\n",
      "recieved tweet # 3\n",
      "recieved tweet # 4\n",
      "recieved tweet # 5\n",
      "recieved tweet # 6\n",
      "recieved tweet # 7\n",
      "recieved tweet # 8\n",
      "recieved tweet # 9\n",
      "recieved tweet # 10\n",
      "recieved tweet # 11\n",
      "recieved tweet # 12\n",
      "recieved tweet # 13\n",
      "recieved tweet # 14\n",
      "recieved tweet # 15\n",
      "recieved tweet # 16\n",
      "recieved tweet # 17\n",
      "recieved tweet # 18\n",
      "recieved tweet # 19\n",
      "recieved tweet # 20\n",
      "recieved tweet # 21\n",
      "recieved tweet # 22\n",
      "recieved tweet # 23\n",
      "recieved tweet # 24\n",
      "recieved tweet # 25\n",
      "recieved tweet # 26\n",
      "recieved tweet # 27\n",
      "recieved tweet # 28\n",
      "recieved tweet # 29\n",
      "recieved tweet # 30\n",
      "recieved tweet # 31\n",
      "recieved tweet # 32\n",
      "recieved tweet # 33\n",
      "recieved tweet # 34\n",
      "recieved tweet # 35\n",
      "recieved tweet # 36\n",
      "recieved tweet # 37\n",
      "recieved tweet # 38\n",
      "recieved tweet # 39\n",
      "recieved tweet # 40\n",
      "recieved tweet # 41\n",
      "recieved tweet # 42\n",
      "recieved tweet # 43\n",
      "recieved tweet # 44\n",
      "recieved tweet # 45\n",
      "recieved tweet # 46\n",
      "recieved tweet # 47\n",
      "recieved tweet # 48\n",
      "recieved tweet # 49\n",
      "recieved tweet # 50\n",
      "recieved tweet # 51\n",
      "recieved tweet # 52\n",
      "recieved tweet # 53\n",
      "recieved tweet # 54\n",
      "recieved tweet # 55\n",
      "recieved tweet # 56\n",
      "recieved tweet # 57\n",
      "recieved tweet # 58\n",
      "recieved tweet # 59\n",
      "recieved tweet # 60\n",
      "recieved tweet # 61\n",
      "recieved tweet # 62\n",
      "recieved tweet # 63\n",
      "recieved tweet # 64\n",
      "recieved tweet # 65\n",
      "recieved tweet # 66\n",
      "recieved tweet # 67\n",
      "recieved tweet # 68\n",
      "recieved tweet # 69\n",
      "recieved tweet # 70\n",
      "recieved tweet # 71\n",
      "recieved tweet # 72\n",
      "recieved tweet # 73\n",
      "recieved tweet # 74\n",
      "recieved tweet # 75\n",
      "recieved tweet # 76\n",
      "recieved tweet # 77\n",
      "recieved tweet # 78\n",
      "recieved tweet # 79\n",
      "recieved tweet # 80\n",
      "recieved tweet # 81\n",
      "recieved tweet # 82\n",
      "recieved tweet # 83\n",
      "recieved tweet # 84\n",
      "recieved tweet # 85\n",
      "recieved tweet # 86\n",
      "recieved tweet # 87\n",
      "recieved tweet # 88\n",
      "recieved tweet # 89\n",
      "recieved tweet # 90\n",
      "recieved tweet # 91\n",
      "recieved tweet # 92\n",
      "recieved tweet # 93\n",
      "recieved tweet # 94\n",
      "recieved tweet # 95\n",
      "recieved tweet # 96\n",
      "recieved tweet # 97\n",
      "recieved tweet # 98\n",
      "recieved tweet # 99\n",
      "recieved tweet # 100\n",
      "recieved tweet # 101\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'status_code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-131846062b31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#starts comsuming public statuses that contain the keyword \"data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\twython\\streaming\\types.pyc\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://stream.twitter.com/%s/statuses/filter.json'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m               \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\twython\\streaming\\api.pyc\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, url, method, params)\u001b[0m\n\u001b[0;32m    152\u001b[0m                                       not valid JSON.')\n\u001b[0;32m    153\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_success\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mmessage_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mmessage_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-131846062b31>\u001b[0m in \u001b[0;36mon_success\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#stop when collected enough\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mprint\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'status_code' is not defined"
     ]
    }
   ],
   "source": [
    "from twython import TwythonStreamer\n",
    "\n",
    "#appending data to a global variable is pretty poor form\n",
    "#but it makes the example much simpler\n",
    "\n",
    "tweets = []\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    \"\"\"Our own subclass of Twython Streamer that specifies how to interact with the stream\"\"\"\n",
    "    \n",
    "    def on_success(self, data):\n",
    "        \"\"\"what to do whentwitter sends us data? Here data will be a python dict representing a tweet\"\"\"\n",
    "        \n",
    "        # only want to collect English-language tweets\n",
    "        if data['lang'] == 'en':\n",
    "            tweets.append(data)\n",
    "            print \"recieved tweet #\", len(tweets)\n",
    "            \n",
    "        #stop when collected enough\n",
    "        if len(tweets)>= 100:\n",
    "            self.disconnect()\n",
    "            \n",
    "    def on_error(self, status_code, data):\n",
    "        print status_code, data\n",
    "        self.disconnect()\n",
    "        \n",
    "stream = MyStreamer(cons_key,cons_secret, access_token, secret_token)\n",
    "\n",
    "\n",
    "#starts comsuming public statuses that contain the keyword \"data\"\n",
    "stream.statuses.filter(track=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @SecurEcomConslt: Consumers worry about their data, but don't bother much with security https://t.co/OYSYl4oFGk\n",
      "\n",
      "#cybersecurity #consume‚Ä¶\n",
      "the update mainly for the current priestess banner and ranking.\n",
      "\n",
      "1st rate up, Yui &amp; Asuna\n",
      "2nd rate up, Sinon &amp; Sugu‚Ä¶ https://t.co/KiBexYo5gX\n",
      "Learn how having a Copy Services Manager simplifies storage-related tasks by automating #storage recovery steps and‚Ä¶ https://t.co/BMJ4HTwRPm\n",
      "\"Don‚Äôt Fall for the Hype: How the FBI‚Äôs Use of Section 702 Surveillance Data Really Works\" https://t.co/0OrOPeGnYQ\n",
      "Great comparison data here - Australia we need to do better to achieve more gender balanced parental leave https://t.co/qAVP9SSdWU\n",
      "RT @Ronald_vanLoon: Which Machine Learning Algorithm Should I Use? | \n",
      "by @kdnuggets @SAS |\n",
      "\n",
      "https://t.co/QQy0I2kqpZ\n",
      "\n",
      "#MachineLearning #Algo‚Ä¶\n",
      "RT @unionsaustralia: Thousands of working families will be worse under the Turnbull Government's childcare changes. #auspol https://t.co/Cf‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "RT @lolayooji: guess it's time for a saved acc to spam votes&amp; radio requests there this acc &amp; the website votes aint enough I'll make it an‚Ä¶\n",
      "ClouDatAIBig Data Marketing Customers Effectively: https://t.co/16yPfLYgsD  Cloud-Native Continuous Integration/Del‚Ä¶ https://t.co/g0ZLMKBERU\n",
      "Learn how having a Copy Services Manager simplifies storage-related tasks by automating #storage recovery steps and‚Ä¶ https://t.co/kgOhwp8qNW\n",
      "RT @rssharma3: My piece on alleged Aadhaar data breach. Comments, only factual and not emotional, are welcome.\n",
      "\n",
      "https://t.co/1kGLPTFgUr\n",
      "RT @marentwickler: 2018's Best New IoT Device Ideas for Data Scientists and Engineers https://t.co/tbWt4LPguR via @bigdatagal\n",
      "State legislatures will be debating dealership data soon: One of the key issues that will be discussed in state... https://t.co/LbDySPulQC\n",
      "State legislatures will be debating dealership data soon: One of the key issues that will be discussed in state... https://t.co/nCKVgkAa0N\n",
      "RT @JamesWRI: #Indigenouspeoples and local communities manage half the world's #land. So why don't they have legal rights to it? New data o‚Ä¶\n",
      "@TelkomZA So you decided to remove the *123#  promotion so what now,bring back the promo or make your data cheap\n",
      "@Denbar64 @rorylane @RanWiz @Jesus_is_G_d @R0samond @ScottBlivtrusci @Caitanyadas1 @geraldpayne25 @umfpt‚Ä¶ https://t.co/qcnwn4lRFo\n",
      "Data Scientist - CoSector - University of London ( South West London, UK )  - [ ‚û° https://t.co/2djKgX7XsL ]‚Ä¶ https://t.co/fvso4KJHWs\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "RT @anne0626: [BLACK &amp; WHITE] 201705 L'officiel Hommes cover with Kim Jae Joong  (2) https://t.co/XUqyGeZuvH https://t.co/jDCgYQnO03 https:‚Ä¶\n",
      "RT @rlingle: Find the latest news in The Predictive Analytics from Sensor Data Daily! https://t.co/wbWpXd7DoW #bigdata #datascience\n",
      "‚ÄúThis utility was designed to calculate date/time values from the various timestamps that may be found inside data‚Ä¶ https://t.co/wcosEBgVeB\n",
      "RT @ktaehoney: Not to be dramatic but I would die for this thread https://t.co/7UmF9zxaZV\n",
      "RT @Asad_Umar: Seeing tweets about provincial tax performance from pml n leaders. Here is the real data from federal Govt finance ministry.‚Ä¶\n",
      "RT @NPR: Recent CDC data show that more than 1 in 4 people who inject drugs reuse needles ‚Äî¬†and many have not had an HIV test in the last y‚Ä¶\n",
      "RT @NPR: Recent CDC data show that more than 1 in 4 people who inject drugs reuse needles ‚Äî¬†and many have not had an HIV test in the last y‚Ä¶\n",
      "RT @AWDtwit: We're getting a Crash Bandicoot artbook this year &lt;3\n",
      "\n",
      "I'm probably one of the only people that gets super excited about game s‚Ä¶\n",
      "Internet of Things and Data Analytics Handbook Published - Amyx+ Internet of Things (IoT) https://t.co/RhGBCYQvqZ #analytics\n",
      "#BigDataRetweet Sensors, location systems, google earth, machine learning algorithms, data‚Ä¶ https://t.co/eonrkgHUsg\n",
      "RT @spignal: INDIA CHART AND DATA EXTRAVAGANZA THREAD. I collated tons of data from various sources for my India's Missing Middle Class pac‚Ä¶\n",
      "@HMOIndia Reminder 16\n",
      "Please Help Me.\n",
      "I have Filed Dec-17 GSTR-3B. But Different Branch Data...\n",
      "By mistake\n",
      "Login ID‚Ä¶ https://t.co/i9VqEqlQN0\n",
      "good thing i‚Äôm no ones wcw üò© https://t.co/nGmZVygBmQ\n",
      "RT @Asamoh_: The two communities (Kikuyus and Kalenjins) according to the data produced by the Ouko team report dominate the Ministries, De‚Ä¶\n",
      "RT @AKermodeBear: #WhatsInTekRoosPouchToday A bitcoin wallet hash chiseled into a stone tablet, a mirror of the data in the NSA's Utah data‚Ä¶\n",
      "I added a video to a @YouTube playlist https://t.co/E6tabJzSIS Jio New Year Plans 2018 | 50% Extra data, Rs.50 Discount\n",
      "RT @Ronald_vanLoon: 10 Big Data Analytics Privacy Problems\n",
      "by @PrivacyProf @SecureWorldExpo |\n",
      "\n",
      "https://t.co/S8RHqdyOwi\n",
      "\n",
      "#BigData #Analytics‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "LuzelleIAlgorithms and Theory of Computation Handbook (Chapman &amp; Hall/CRC Applied Algorithms and Data Structures se‚Ä¶ https://t.co/cwKBPoWvuC\n",
      "RT @AngelaSterritt: Set up in this office today covering a data-thon where dozens of data scientists are collecting information about overd‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "Mordaunt: Britain will cut aid spending to richer nations https://t.co/R2oEGb4fCt\n",
      "RT @funnymonkey: Instagram taps into user insecurities to get users to return.\n",
      "\n",
      "Of course, to do this, they must first mine their data set‚Ä¶\n",
      "RT @funnymonkey: Instagram taps into user insecurities to get users to return.\n",
      "\n",
      "Of course, to do this, they must first mine their data set‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "RT @roadscholarz: report in Sandesh re biometric data leak in Guj PDS: https://t.co/9TCCw64zGw\n",
      "see  this part: https://t.co/KLkXqGV2H6 a pi‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "Big Data Meets Data Fabric and Multi-cloud - Forbes https://t.co/C6LR0qVakE\n",
      "RT @benwikler: The Democratic Legislative Campaign Committee is the one national entity focused on electing Democrats to *state legislature‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "RT @TheDLCC: Come work for the best team in politics! The DLCC is hiring:\n",
      "Data Coordinator\n",
      "Deputy National Field Director\n",
      "Executive Assista‚Ä¶\n",
      "RT @DentsuAegisANZ: Dentsu Aegis Network Ad Spend Forecasts, based on data from 59 markets, puts global growth at 3.6% in 2018, up from 3.1‚Ä¶\n",
      "RT @tcs_na: Learn how retailers are leveraging A.I. and big data technologies to achieve insight driven omni-channel assortment. We're at b‚Ä¶\n",
      "RT @MollyMcKew: I've noticed there's a correlation between friends who claim they don't like reading science fiction and friends who have i‚Ä¶\n",
      "RT @ProPublica: Last year we reported about how and why law enforcement agencies mishandle hate crime data. \n",
      "\n",
      "We have opened up the records‚Ä¶\n",
      "RT @repu_x: \"If #data is ‚Äúthe new oil‚Äù, it will be deposited in the #blockchain.\"\n",
      "\n",
      "#bigdata #dApps #datamarketplace #RepuX\n",
      "\n",
      "https://t.co/Rf‚Ä¶\n",
      "RT @JackPosobiec: We‚Äôve seen a lot of unsealed FBI search warrants from Las Vegas, but where‚Äôs the MGM warrants to seize all the video, CCT‚Ä¶\n",
      "RT @GedKACTU: So plain to see where Turnbull‚Äôs priorities lie - Banks, Wealthy Landlords, Big Corps, Well off families. ‚ÄòSecret‚Äô data shows‚Ä¶\n",
      "RT @3liza: i know i keep harping on about the black box shit the social networks are doing but really, pay attention, and make mental adjus‚Ä¶\n",
      "RT @3liza: i know i keep harping on about the black box shit the social networks are doing but really, pay attention, and make mental adjus‚Ä¶\n",
      "RT @Ronald_vanLoon: Top Algorithms and Methods Used by Data Scientists\n",
      "by @kdnuggets |\n",
      "\n",
      "https://t.co/5jqwusAIJX\n",
      "\n",
      "#DataScience #DataScientis‚Ä¶\n",
      "RT @3liza: i know i keep harping on about the black box shit the social networks are doing but really, pay attention, and make mental adjus‚Ä¶\n",
      "RT @engadget: Apple Health app data used as evidence in rape investigation https://t.co/uDItmBMIx8\n",
      "‚Ä¢ Build evaluations around key objectives to find out how audiences were impacted\n",
      "‚Ä¢ Capture public data to add evid‚Ä¶ https://t.co/VqqFwar31N\n",
      "RT @MollyMcKew: I've noticed there's a correlation between friends who claim they don't like reading science fiction and friends who have i‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "RT @Ronald_vanLoon: Top Algorithms and Methods Used by Data Scientists\n",
      "by @kdnuggets |\n",
      "\n",
      "https://t.co/5jqwusAIJX\n",
      "\n",
      "#DataScience #DataScientis‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "RT @beyonddc: Here is the link to the raw data. Help me fill out the remaining holes!\n",
      "https://t.co/Ea76efbwue\n",
      "DE-CIX New York Expands Into Two Telx New York City Data Centers | 2014-05-21 | Mission Critical Magazine https://t.co/YvlphIIEBH\n",
      "RT @tomilapinlampi: The #opendata from @Liikennevirasto is bubbling again, thanks to @tjukanov https://t.co/jdR2MkMgyZ\n",
      "RT @beyonddc: Here are the session notes, including bullets of the 11 lessons, the powerpoint, and the raw data. #transpo18 \n",
      "https://t.co/d‚Ä¶\n",
      "RT @metamedio: Business must tone down its lust for big data https://t.co/dx91Tay4ao #OpenData\n",
      "RT @windscribecom: New server locations added: Philadelphia, New Orleans, Baltimore and Milwaukee. This brings the total # of data-centers‚Ä¶\n",
      "@gorskon @LidaAHall DATA:\n",
      "\"Blood-thirsty criminals kill cancer patients en masse\"\n",
      "https://t.co/9uGnrB1znl\n",
      "The files‚Ä¶ https://t.co/lrqcKrcDh3\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "RT @FlyoutChase_: Me when I go to the genius bar and they tell me I need to replace my doll after being with her for 2 years and having 10T‚Ä¶\n",
      "Shasta Lake, CA : Mount Shasta, Lake Shasta, Shasta Dam photo, picture, image (California) at‚Ä¶ https://t.co/776MTxxelb\n",
      "RT @_Rysheen: They said ‚ÄòLoso you leaving the crib tonight?‚Äù \n",
      "Told ‚Äòem I‚Äôm giving shorty my data. Loading her with gigabytes https://t.co/u‚Ä¶\n",
      "RT @OracleDatabase: Learn how to build an #Oracle #JavaScript Extension Toolkit (Oracle JET) CRUD application that interacts with #OracleDa‚Ä¶\n",
      "RT @metamedio: A series of initiatives from the United Nations to enhance use of big data for driving sustainable ... https://t.co/VN4zIbgZ‚Ä¶\n",
      "A criminal treasonous attempt at a pre &amp; post election coup. Book 'em, Dano. What did Mueller, Obama know? https://t.co/3OibtnvcbT\n",
      "#Jobs #JobSearch  Senior Engineer - Data Scientist Openings @ #Bangalore https://t.co/beidgPbo1r #datascientist\n",
      "RT @shizziebizz: @_sdawgy @Telstra Should of went to @VodafoneAU \n",
      "More data bang üí• for your buck üí∞\n",
      "With great data comes great MONEY.\n",
      "RT @WritersEdit: How Data Can Help You #Write A Better #Screenplay by @WaltHickey -&gt; https://t.co/hKYscAdJZ4 #writerslife\n",
      "Interact with ProTaiga's CEO &amp; Founder, Aval Sethi at the Clean India Show event in Mumbai on 20th Jan, 2018. He is‚Ä¶ https://t.co/cCiGyRRtcO\n",
      "@TIME this whole sequence of events is ... I can't even imagine ... every thought, from every member of the populat‚Ä¶ https://t.co/6bL9gh150p\n",
      "RT @funnymonkey: Instagram taps into user insecurities to get users to return.\n",
      "\n",
      "Of course, to do this, they must first mine their data set‚Ä¶\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "[2017-DEC] Automatic Data Processing, Inc. ADP Correlation Histogram #bigdata https://t.co/oxLnMHt5sY\n",
      "RT @MollyMcKew: I've noticed there's a correlation between friends who claim they don't like reading science fiction and friends who have i‚Ä¶\n",
      "How to Become a Better Data-Informed Content Marketer https://t.co/Vjiy19HRsq\n",
      "RT @MAPS_ME: Offline maps for FREE! Use in hometown and abroad. No cellular data is required! https://t.co/WStshoqffe\n",
      "Digital Marketing News: What Marketers Think about AI, Autonomous Stores &amp; GSC Adds Data https://t.co/WNKDjql4ah\n",
      "RT @MedStillCold: ‚ÄúThe crew gave her pipe but you wanted to date her, ya sex doll for everybody nigga that‚Äôs unlimited data‚Äù https://t.co/N‚Ä¶\n",
      "Agreed https://t.co/nPtTQJ5zaN\n",
      "RT @DeepLearn007: Learning About Machine Learning  \n",
      "#AI #MachineLearning #BigData #Fintech #ML #Security #tech \n",
      "https://t.co/U6IoP1fkGa htt‚Ä¶\n",
      "Hey @neiltyson, what's that cluster of stars that's about 1/3 from Orion to (what I hope I correctly remember is) C‚Ä¶ https://t.co/estt0xAiSI\n",
      "Great presentation by Elton de Souza from IBM innovation lab on IBM LinuxONE - exceptional digital experimence thro‚Ä¶ https://t.co/eMRq14h1k6\n",
      "RT @FoxBusiness: .@RandPaul on Section 702 of FISA: We're asking for two things: This big, massive amount of data that has innocent America‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "for text in [tweet['text'] for tweet in tweets]:\n",
    "    print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_hashtags = Counter(hashtag['text'].lower()\n",
    "                       for tw\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
